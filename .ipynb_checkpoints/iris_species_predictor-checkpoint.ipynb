{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5b6879-aed5-4c7b-8878-03b73c7dfb52",
   "metadata": {},
   "source": [
    "# The Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f40864-40fe-41b3-bd20-70f21154d761",
   "metadata": {},
   "source": [
    "The iris dataset is a famous dataset taken from Fisher's paper. Iris is a flowering plant and the common name given to all iris species. \n",
    "\n",
    "Fisher's dataset contains data about 3 classes/species of iris plant (iris-setosa, iris-versicolor and iris-virginica). 150 plants were sampled, 50 for each species of iris. Four features (individual x variables) were measured: sepal length, sepal width, petal length and petal width, all in cm. The class of iris plant was the target (y variable) which had been assigned by an expert botanist. Thus, this is an example of supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0b04b-8348-4234-a7b1-a747d42f77f0",
   "metadata": {},
   "source": [
    "Importing in an image that shows the different species of iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1c31b-36ee-47ce-a1dd-df211e004dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4595d409-9fa4-47b7-b138-49bcc17431b7",
   "metadata": {},
   "source": [
    "Aim: The aim of this code is to produce a model using the iris dataset, to determine the species of iris based on these certain features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0bc80-9796-4651-9b04-2d0d4642b912",
   "metadata": {},
   "source": [
    "**Importing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca3ea6-c4eb-41ce-b1a6-dce19b3c648f",
   "metadata": {},
   "source": [
    "Scikit-learn contains a few datasets (including iris). Therefore, scikit-learn can be loaded and the iris dataset imported from this, rather than having to download it from an external website. \n",
    "\n",
    "Defining and splitting the data into X and y: \n",
    "\n",
    "- X (capital) is given for all the features (sepal length, width and petal length, width) \n",
    "\n",
    "- y is given for the target (iris class - setosa, verscolor and virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6c1f0-3e01-49c8-a80f-7705da0985fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ab219-b6ea-4080-a863-5b76ff2df768",
   "metadata": {},
   "source": [
    "**Examining the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9982d76-e4ad-4b7d-8ad1-feede63c5f4b",
   "metadata": {},
   "source": [
    "First looking at the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572275e-345b-424d-8b27-60370e7b3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.replace(dict(enumerate(load_iris().target_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44268e9-df7d-446d-a640-e1028410a4ae",
   "metadata": {},
   "source": [
    "This funciton above converts the numerical target values (0, 1, 2) to strings, the species name (setosa, versicolor, virignica), respecitvely, for the y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b84e4-13a4-40f5-8189-c7a7bf78e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc914c-f5d7-4ea9-8bc1-50cd4fffd4df",
   "metadata": {},
   "source": [
    "Looking at the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23b6d1-4600-4375-b33a-b28807f4de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c43b79-8fa3-4442-a5dd-335564f2f863",
   "metadata": {},
   "source": [
    "This table shows the 4 features and the data for the first 5 samples of iris."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6e2d3-52cc-4cd6-967b-5691444be07a",
   "metadata": {},
   "source": [
    "**Plotting the correlation between features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28378b-4c83-4e2f-bb4d-41b230e9039a",
   "metadata": {},
   "source": [
    "To get the linear correlation between all the features, the corr() method is called on the features (X) and assigned to the variable corr. The table sets out the features along the rows and columns, giving the correlation between each pair. 1.0 is given as the absolute positive correlation and thus, shows for the correlation between the same feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee83fcb-498a-416d-9bba-df0217657a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e3077-f350-4f69-bb21-551c7690ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15774b02-9243-40d1-8665-52da292d8e0a",
   "metadata": {},
   "source": [
    "It is useful to see the correlation visually. Thus, a heatmap and pairplot can be plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc6f5c-6e51-4083-b00e-083d7306115f",
   "metadata": {},
   "source": [
    "Plotting a heatmap:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e68184-2727-4244-8f06-4a01ee4bbad8",
   "metadata": {},
   "source": [
    "The visualisation library Seaborn, is imported and the heatmap method applied to the correlation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e7074-b979-4ee8-8825-f51d0b934e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(corr, vmin=-1.0, vmax=1.0, square=True, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f251fa-05b9-443a-a459-03a1ba5b3944",
   "metadata": {},
   "source": [
    "The heatmap is an example of multiple cross-correlation and plots every feature against each other, colouring in the cell based on the correlation between the features. A first glance shows sepal width and sepal length are the most uncorrelated pair. In comparison, petal width and petal length are the most positively correlated. Petal length and sepal width are the most negatively correlated pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47bec2-81b1-4ea5-94d1-9aec84d73bc0",
   "metadata": {},
   "source": [
    "Plotting a pairplot:\n",
    "\n",
    "pandas is imported and the theme is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61a921-7ef9-444b-9c55-de62638c3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b0a62-2ab8-414f-a1ae-df2136c21c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\", palette=\"husl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e486914-40d8-436a-a1e5-b6f3619e2021",
   "metadata": {},
   "source": [
    "For the purpose of plotting a pairplot and scatter plot the X and y data needs to be grouped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191a07c-0a86-402d-8cd6-a53231e55711",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = pd.concat([X,y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43449c92-7aa5-477d-8122-bcb849332c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=X_y, hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe3a78-2062-47fc-a7b6-8e0e78efa7ad",
   "metadata": {},
   "source": [
    "The pairplot plots each feature against every other and each cell intersection shows a scatter plot to indicate the relationship between the two features. The plots are also coloured by species of iris. It confirms that sepal width and sepal length are the most uncorrelated features as the datapoints are more scattered forming a blob rather than a strong diagonal line. Therefore, it seems that the two best features to use to make a model which can predict the species of iris is sepal width and sepal length, since both features are providing predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ce9bf-4b9b-449d-8ce0-8206865a9e84",
   "metadata": {},
   "source": [
    "Looking at the indivdual scatter plot for sepal length against sepal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24343df-5e6f-40df-8a4c-f07e7c563229",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=X_y, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c27c84-b234-4e87-8886-e6b306d7885c",
   "metadata": {},
   "source": [
    "This scatter plot is the same as in the pairplot but blown up. It shows clearly that the two features are fairly uncorrelated. The species look fairly distinct from each other with setosa being well separated from versicolor and virginica. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a1b4f-1412-4dbd-848f-5971ab1d78c8",
   "metadata": {},
   "source": [
    "**Making a predictive model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92badd-8e5d-481c-bb1e-402135b1ec27",
   "metadata": {},
   "source": [
    "**k-nearest neighbours (kNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f1f1b-728f-49b2-b184-337c847be18b",
   "metadata": {},
   "source": [
    "To make a model which can predict the species of iris based on sepal length and sepal width, k nearest neighbours can be used. This will work well as based on sepal length and width, the species are relatively uncorrelated and distinguishable from each other.\n",
    "\n",
    "kNN works by assigning the species of iris to your data, based on the most common species of its nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b6e84-b186-4758-8e2b-72898fd2fae1",
   "metadata": {},
   "source": [
    "First the dataframe is defined and the two desired features selected. In this case they are sepal length and sepal width. Here XS is used to represent X-Subset.\n",
    "\n",
    "The data is then split into a train and test subset. Here the random split is assigned as 42 so that the random split is reproducible each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbc118-2be0-4e99-96f0-865a03e0f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = DataFrame(load_iris().data, columns=load_iris().feature_names)\n",
    "XS = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]\n",
    "y = load_iris().target\n",
    "\n",
    "train_XS, test_XS, train_yS, test_yS = train_test_split(XS, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4e93f-f09e-4ebe-8ece-68172e3ea2ca",
   "metadata": {},
   "source": [
    "The model can now be used and to do this, the model KNeighboursClassifier needs to be imported. For now, the number of neighbours is set to the default, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cee65f-3064-4a17-a9cc-3cbeb007dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(train_XS, train_yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7ae60-af8f-4d21-bc03-f465e7021796",
   "metadata": {},
   "source": [
    "The performance of the model can be checked against the test data, producing a score which assesses how good the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf6ede-0ac6-46e4-b808-236954c9f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(test_XS, test_yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3b3f2-a366-4419-9ac4-eea082285f5b",
   "metadata": {},
   "source": [
    "This score suggests the model is pretty good but there is still room for improvement.\n",
    "\n",
    "The hyperparameter (number of neighbours) can be adjusted to give the ideal number of nearest neighbours which produce the best possible score. This can be done using GridSearchCV which will automatically run every possible hyperparameter it is given, so the best one can be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1a618-f916-4045-b089-255c68bf9255",
   "metadata": {},
   "source": [
    "In this case, it will run every value of neighbours from 1 to 60, using the training data to choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2721ff-9b1d-40dd-90aa-c8e4aa76f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "parameters = {\n",
    "    \"n_neighbors\" : range(1, 60),\n",
    "}\n",
    "model = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "model.fit(train_XS, train_yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f273853-d297-457c-ba35-f3165fcd4619",
   "metadata": {},
   "source": [
    "This sorts the analysis by the test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b0074-46ab-4b7c-a54d-c22389ec73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = DataFrame(model.cv_results_)\n",
    "cv_results = cv_results.sort_values([\"rank_test_score\", \"mean_test_score\"])\n",
    "cv_results.head()[[\"param_n_neighbors\", \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abd756-e9e9-4126-8308-859630612bec",
   "metadata": {},
   "source": [
    "This table implies that 18 and 31 neighbours are the best ranked that give rise to the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294d68f-0b55-4e70-adf9-07236beee14b",
   "metadata": {},
   "source": [
    "A scatter plot can also be plotted to show this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf2490-66fc-4d91-aee3-68f9592c229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c901c-ef94-4875-80de-4d01c4fb1be2",
   "metadata": {},
   "source": [
    "From both the table and plot, 18 or 31 seems like the best number of nearest neighbours to use. The model can now be fitted with with both 18 and 31 nearest neighbours to see which is the best number to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f86f95-926b-4bee-b802-63c590494571",
   "metadata": {},
   "source": [
    "First with 18 nearest neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f5b28-23e0-4924-8ff4-d3e439cb249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=18)\n",
    "model.fit(train_XS, train_yS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc3ee4-5dca-47d1-9680-94d5ac44d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(test_XS, test_yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7aca5-9198-4926-a26b-27fc624b42fe",
   "metadata": {},
   "source": [
    "This gives a lower score compared to when run with the default, 5 nearest neighours.\n",
    "\n",
    "Now fitting with 31 nearest neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82c4e5-777f-4585-b360-139628bd3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=31)\n",
    "model.fit(train_XS, train_yS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ae9fe-6d80-42f4-bb7b-c879e43c04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(test_XS, test_yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993576fd-c307-41d2-8cd7-b33928300e21",
   "metadata": {},
   "source": [
    "This shows that using 31 nearest neighbours gives a better model than with 18 and 5. It improves the model, increasing the score to 0.895 (3 d.p.). Again, this is a good score but can still be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7414add6-a085-4d4f-8b97-f25cc0d51869",
   "metadata": {},
   "source": [
    "This model can then be plotted by creating a plot.py module with the relavent code and importing this in.\n",
    "\n",
    "The plot shows the nearest neighbours classification and plots the decision boundaries for each species when, k = 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c35efa-9a7d-461e-a900-2a6be904aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import plot_knn\n",
    "\n",
    "plot_knn(model, XS, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e881529-860d-4a9e-87d2-bf0b04cc7a0f",
   "metadata": {},
   "source": [
    "As before, this KNN plot of our model shows that setosa is well separated from both versicolor and virginica. However, there is some overlap between versicolor and virginica and they are not as well separated from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e214e27-742b-4336-91bd-fada44feefb9",
   "metadata": {},
   "source": [
    "**Modifying the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0679f94-dd72-4874-bd9e-b4879d4a6584",
   "metadata": {},
   "source": [
    "**Feature scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702cd9e-e549-4e93-8aed-9aa7b1140664",
   "metadata": {},
   "source": [
    "The values for sepal length and sepal width have different ranges but they are not hugely different. A big difference in ranges can impact kNN. To combat this is to scales the values. In this case feature scaling can be adopted to see what difference it makes to the ranges and subsequently the model score. The StandardScaler can be imported from skikit-learn and perform scaling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cc5ab-da90-4bf1-b00f-c30001c2468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(XS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a5b5d-8e43-4d6b-806f-0094120e2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "XS_scaled_raw = scaler.transform(XS)\n",
    "XS_scaled = pd.DataFrame(XS_scaled_raw, columns=XS.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9c799-b4c5-4ef3-a002-127aee43639d",
   "metadata": {},
   "source": [
    "The scaled results can then be plotted as a scatter plot to see the changes in range to sepal length and sepal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fb333-78f5-4765-8793-f74075ba4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=XS_scaled, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=y, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37a173-9215-4d26-8435-0d103c850df6",
   "metadata": {},
   "source": [
    "After being scaled the values exist in the same ranges and thus there is an even spread in the data.\n",
    "\n",
    "Feature scaling can then be added into the model by making a pipeline which uses the scaled data and calls kNN on this. This is then run and a model score generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff562a-8f16-4015-91b2-d8f0c16b7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaled_knn_XS = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=31)\n",
    ")\n",
    "scaled_knn_XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf41f2b-215d-44fd-94e1-80423adc7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_knn_XS.fit(train_XS, train_yS)\n",
    "scaled_knn_XS.score(test_XS, test_yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf374901-5451-4b3a-9cb7-3bd2605cd2c3",
   "metadata": {},
   "source": [
    "The score is in fact slightly worse compared to the non-scaled data with k=31. The boundaries can then be plotted again by calling the plot_knn function from the previously created module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f621a1-e6a5-402d-980e-5be31775955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn(scaled_knn_XS, XS, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ca059-e018-4369-bea0-6d54f0e43bd1",
   "metadata": {},
   "source": [
    "Comparing this to the plot for the non-scaled data it can be observed that there is only a subtle change with the boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc19563-30dc-431a-b12d-11fc813b9864",
   "metadata": {},
   "source": [
    "**Applying the model to all the features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2446f7-6bed-4b9e-8bc1-b17287211fcf",
   "metadata": {},
   "source": [
    "This pipeline (scaling and kNN) can now be applied to all the features rather than just two (sepal length and sepal width).The data first is re-split, this time using all the features and then the pipeline is applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2e2cd-1f13-4234-ba33-ee6790bf0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3b213-c8ce-43d0-b2cb-e80f2edfff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_all_X = make_pipeline(    \n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier()\n",
    ")\n",
    "knn_all_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82251fba-099b-4fec-ae0b-e20b77186a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_all_X.fit(train_X, train_y)\n",
    "knn_all_X.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f872e2d-945c-428a-8f81-d5252aa4b05e",
   "metadata": {},
   "source": [
    "The score for this is too perfect. A PCA can be done to reduce the dimensionality whilst still retaining enough information about the data as a whole. This should help improve the score and make it more realistic and thus, not too perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d1aee-4d85-4cf3-af2f-867354797cd8",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343c735-2b19-4f44-87a0-6025d4b82398",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) can be done to indentify the most important features or to group highly correlated features togther in a principal component. This can then reduce the dimensionality and allow more than two features to be included in the model which may improve the score and better represent the data as a whole. \n",
    "\n",
    "The PCA function can be imported and then incorporated into the pipeline. It is important to note that the number of components is the same as the number of features in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9d31b-42e8-410e-b365-038bc34b26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_knn_X = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=4),  \n",
    "    KNeighborsClassifier(n_neighbors=31)\n",
    ")\n",
    "pca_knn_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184259e-4350-4861-9d27-c99437b1bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_knn_X.fit(train_X, train_y)\n",
    "pca_knn_X.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad25356-d2cd-43a4-8f04-cf3700ee0443",
   "metadata": {},
   "source": [
    "The score has now improved since it is no longer exactly 1.0. Therefore, emplying PCA has seemed to help the model and make it a better predictor. \n",
    "\n",
    "The amount of variation explained by each principal component (PC) can then be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fa53d-684f-4545-aab6-a615f057e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_knn_X[\"pca\"].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86028a-4be3-4e08-981e-1d07c69b551e",
   "metadata": {},
   "source": [
    "This output reveals that the frist principal component provides 72%, the second 24%, the thrid 4% and the fourth 0.6%. Thus, the first two PC's explain most of the variation. It can be seen that all four components, together explain 100% of the variation, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251aac8-9289-4b2c-823f-04aeb5642b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca_knn_X[\"pca\"].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f4803-a0c5-4709-81be-d1523a90b940",
   "metadata": {},
   "source": [
    "GridSearchCV can be employed again to try different numbers of PC's to see how many components give the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29582595-b9ee-4c45-b606-3124e433e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pca_knn_cv = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        PCA(),\n",
    "        KNeighborsClassifier(n_neighbors=31)\n",
    "    ),\n",
    "    {\n",
    "        \"pca__n_components\" : range(1, 5),\n",
    "    }\n",
    ")\n",
    "pca_knn_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637da9ca-d16d-4410-8c50-50e2ce7fe8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_knn_cv.fit(train_X, train_y)\n",
    "pca_knn_cv.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02a206-f426-4172-82c5-cd284b5ef1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_knn_cv.best_estimator_[\"pca\"].n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dbb6f-9e15-4ef9-8eef-864d8955c71b",
   "metadata": {},
   "source": [
    "This shows that 4 PC's are best to use. As before we can then run the pipeline with scaled data, 4 PC's and 31 nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f50cd-e56c-4538-9654-d15072b4abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_knn_2 = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=4),  \n",
    "    KNeighborsClassifier(n_neighbors=31)\n",
    ")\n",
    "pca_knn_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aeacae-d3da-4531-b0e3-d9703e033298",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_knn_2.fit(train_X, train_y)\n",
    "pca_knn_2.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b252ce5-c88e-4a77-97e9-6c4e1c8ce290",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca_knn_2[\"pca\"].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e8bb3-506b-4501-89d5-86cc80c03778",
   "metadata": {},
   "source": [
    "The score is 0.947 (3 d.p.) which is good and is an improved score from the first kNN model with 5 nearest neighbours, a model with just 31 nearest neighbours and a model which did not include PCA. The model incorporates scaled data and principal components that explain all the variation in the data. This is good as it takes all the collected data into account rather than just a selected part of it. Thus, PCA has helped reduce the dimesionality and incorporated more than two features into the model. Overall, the model is a good predictor for the species of iris of the plant and has a distinct separation between setosa and both versicolor and virginica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4666a-d25a-481d-abda-7e0556c616e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
